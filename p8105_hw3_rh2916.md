p8105\_hw3\_rh2916
================
Rui Huang
October 6, 2018

## Problem 1

``` r
library(p8105.datasets)
data(brfss_smart2010)
```

``` r
problem1_data =
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health" ) %>%
  mutate(response = as.factor(response)) %>%
  select(-class, -topic, -question, -sample_size, -(confidence_limit_low : geo_location))
```

### In 2002, which states were observed at 7 locations?

``` r
problem1_data %>%
  filter(year == 2002) %>%
  group_by(locationabbr) %>%
  summarize(n_locationdesc = n_distinct(locationdesc)) %>%
  filter(n_locationdesc == 7)
```

    ## # A tibble: 3 x 2
    ##   locationabbr n_locationdesc
    ##   <chr>                 <int>
    ## 1 CT                        7
    ## 2 FL                        7
    ## 3 NC                        7

Based on the result, CT, FL and NC were observed at 7 locations in
2002.

### Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

``` r
problem1_data %>%
  group_by(year, locationabbr) %>%
  summarize(n_obs = n()) %>%
  ggplot(aes(x = year, y = n_obs, color = locationabbr)) +
  geom_line() +
  labs(
    title = "Spaghetti plot for number of observations in each state",
    x = "Year",
    y = "Number of observations",
    caption = "Data from the rnoaa package"
  ) 
```

![](p8105_hw3_rh2916_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

Based on the result, we can find that the fluctuation for observations
increases by year. And the maximum observation number is about 220,
which occurs in 2007. The majority of observation numbers are under 100.
However, it is difficult to tell the observaion differences between
different states by the spaghetti
plot.

### Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

``` r
knitr::kable(
  problem1_data %>%
  spread(key = response, value = data_value) %>%
  janitor::clean_names() %>%
  filter((year == 2002 | year == 2006 |year == 2010) & locationabbr == "NY") %>%
  group_by(year) %>%
  summarize(mean_prop_excellent = mean(excellent/100, na.rm = T),
            sd_prop_excellent = sd(excellent/100, na.rm = T))
)
```

| year | mean\_prop\_excellent | sd\_prop\_excellent |
| ---: | --------------------: | ------------------: |
| 2002 |             0.2404000 |           0.0448642 |
| 2006 |             0.2253333 |           0.0400083 |
| 2010 |             0.2270000 |           0.0356721 |

From the table we can find that the mean proportion of excellent in 2002
is higher than 2006 and 2010, while the sd of proportion of excellent in
2002 is also the highest. The mean proportion of excellent in 2006 nd
2010 are
similar.

### For each year and state, compute the average proportion in each response category. Make a five-panel plot that shows the distribution of these state-level averages over time.

``` r
problem1_data %>%
  group_by(year, locationabbr, response) %>%
  summarise(state_mean = mean(data_value)/100) %>%
  ggplot(aes(x = year, y = state_mean, color = response)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_grid(. ~response) +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Distribution of state-level average responses over time",
    x = "Year",
    y = "State-level average responses"
  )
```

    ## `geom_smooth()` using method = 'loess' and formula 'y ~ x'

    ## Warning: Removed 21 rows containing non-finite values (stat_smooth).

    ## Warning: Removed 21 rows containing missing values (geom_point).

![](p8105_hw3_rh2916_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

From the plot, we can find that for each year the proportion for very
good is the highest an the proportion for poor is the lowest. The
proportion distribution for this 9 years are similar.

## Problem 2

``` r
library(p8105.datasets)
data(instacart)
Problem2_data = 
  instacart %>%
  janitor::clean_names()
mode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
}
```

### Write a short description of the dataset, noting the size and structure of the data.

The size of the dataset is 1384617, 15, containing 1384617 rows
(observations) and 15 columns (variables), which are order\_id,
product\_id, add\_to\_cart\_order, reordered user\_id eval\_set
order\_number order\_dow order\_hour\_of\_day,
days\_since\_prior\_order, product\_name, aisle\_id, department\_id,
aisle and department. This dataset contains integer, character variable.
There is information on users, orders, products, thus, the key variables
are product\_name(product\_id), aisle(aisle\_id) and
department(department\_id). From the key variables, we can tell the
information about each order product. For instance, we can know that the
most order is Banana for 18726, it is “fresh fruits” and is from
department
“produce”.

### How many aisles are there, and which aisles are the most items ordered from?

``` r
Problem2_data %>%
  distinct(aisle) %>% 
  nrow()
```

    ## [1] 134

``` r
mode(Problem2_data$aisle)
```

    ## [1] "fresh vegetables"

From the result, we know that there are 134 aisles and ’fresh
vegetables" is the most item ordered.

### Make a plot that shows the number of items ordered in each aisle.

``` r
Problem2_data %>%
  mutate(aisle_id = as.character(aisle_id)) %>%
  group_by(aisle_id) %>%
  summarize(n_items = n_distinct(order_id)) %>%
  mutate(
    aisle_id = forcats::fct_reorder(aisle_id, n_items))%>% 
  ggplot(aes(x = aisle_id, y = n_items)) + 
  geom_point() +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle",
    y = "Number of items"
  ) 
```

![](p8105_hw3_rh2916_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

From the plot, we can find that aisle “Banana”(id 24) has the maximum
number of order for over 70000 items. The majority of aisles have order
number under 2000. This plot is ranked by number of
items.

### Make a table showing the most popular item aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”

``` r
knitr::kable(
  Problem2_data %>%
  filter(aisle == 'baking ingredients'|aisle== 'dog food care'|aisle=='packaged vegetables fruits') %>%
  group_by(aisle,product_name) %>%
  summarize(n_product = n()) %>% 
    group_by(aisle) %>% 
    filter(n_product==max(n_product))
)
```

| aisle                      | product\_name                                 | n\_product |
| :------------------------- | :-------------------------------------------- | ---------: |
| baking ingredients         | Light Brown Sugar                             |        499 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |         30 |
| packaged vegetables fruits | Organic Baby Spinach                          |       9784 |

The most popular item aisles “baking ingredients”, “dog food care”, and
“packaged vegetables fruits” are “Light Brown Sugar”, “Snack Sticks
Chicken & Rice Recipe Dog Treats” and “Organic Baby
Spinach”.

### Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

``` r
knitr::kable(
  Problem2_data %>%
  filter(product_name == "Pink Lady Apples"|product_name=="Coffee Ice Cream") %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_order_hour = mean(order_hour_of_day)) %>%
  spread(key = order_dow, value = mean_order_hour) %>%
  rename( "Sunday" = "0",
          "Monday" = "1",
          "Tuesday" = "2",
          "Wednesday" ="3",
          "Thursday" = "4",
          "Friday" = "5",
          "Saturday" = "6")
)
```

| product\_name    |   Sunday |   Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday |
| :--------------- | -------: | -------: | -------: | --------: | -------: | -------: | -------: |
| Coffee Ice Cream | 13.77419 | 14.31579 | 15.38095 |  15.31818 | 15.21739 | 12.26316 | 13.83333 |
| Pink Lady Apples | 13.44118 | 11.36000 | 11.70213 |  14.25000 | 11.55172 | 12.78431 | 11.93750 |

From the table we can find that for “Coffee Ice Cream”, the most order
time is Tuesday and the least is in Friday. For “Pink Lady Apples”, the
most order time is Wednesday and the least is Monday.

## Problem 3

``` r
library(p8105.datasets)
data(ny_noaa)
```

### The goal is to do some exploration of this dataset. To that end, write a short description of the dataset.

The size of the dataset is 2595176, 7, there are 2595176
rows(observations) and 7 columns(variables), which are id, prcp, snow,
snwd, tmax, tmin and date. This dataset contains date, integer,
character variable. There is information onstation, observation date,
precipitation, snow and temperature, thus, the key variables are id,
date, prcp, snow and tmax, tmin. From the key variables, we can tell the
information about the weather in each station everday. For instance, we
can know that in 01/03/1981, there are no precipitian and snow at
00300023 and the maximux and minimum temperature are -12.2 and -20.6
degrees C. There are 145838 missing values for precipitation, 381221
missing values for snowfall, 591786 missing values for snow depth,
1134358 missing values for tmax,1134420 missing values for tmin. We can
find that temperature have more missing values, which is 0.4371264 and
0.4371264of the whole
observations.

### Do some data cleaning. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units.

``` r
Problem3_data = 
  ny_noaa %>%
  janitor::clean_names() %>%
  separate(date, into = c("year", "month", "day"), sep = "-") %>%
  mutate(tmax = as.numeric(tmax),
         tmin = as.numeric(tmin),
         prcp = prcp/10
         ) %>%
  mutate(tmax=tmax/10,
         tmin = tmin/10) 
```

Change units for maximum and minimum temperature from tenth of degree C
to degree C and precipitation, from tenth of mm to mm.

### For snowfall, what are the most commonly observed values? Why?

``` r
mode(Problem3_data$snow)
```

    ## [1] 0

0 is the most commonly observed value. That is because for the majority
of time, there is no snow in New
York.

### Make a two-panel plot showing the average temperature in January and in July in each station across years.

``` r
Problem3_data %>%
  filter((month == "01" | month == "07") & !is.na(tmax)) %>%
  group_by(month, year, id) %>%
  summarize(mean_temp = mean(tmax)) %>% 
  ggplot(aes(x = year, y = mean_temp)) +
  geom_boxplot() +
  facet_grid(.~month) +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Average maximum temperature in January and in July in each station across years",
    x = "Year",
    y = "Average temperature"
  )
```

![](p8105_hw3_rh2916_files/figure-gfm/unnamed-chunk-15-1.png)<!-- -->

From the plot we can find that July has highermaximum temperature than
January. There are outliers and it show that there are some abnormally
high temperature at January and abnormally low temperature at
July.

### Make a two-panel plot showing (i) tmax vs tmin for the full dataset (ii) the distribution of snowfall values greater than 0 and less than 100 separately by year.

``` r
plot1 = 
  Problem3_data %>%
  filter(!is.na(tmax) & !is.na(tmin)) %>%
  ggplot(aes(x = tmax, y = tmin)) + 
  geom_hex() +
  labs(
    title = "tmax vs tmin for the full dataset", 
    x = "tmax",
    y = "tmin" ) +
  theme(legend.position = "bottom", legend.key.width = unit(1.2, "cm"))

plot2 = 
  Problem3_data %>%
  filter(snow > 0 & snow < 100) %>%
  ggplot(aes(x = snow, color = year)) + 
  geom_density(alpha = .4, adjust = .5) +
  labs(
    title = "The distribution of snowfall values", 
    x = "Snowfall values",
    y = "Density" ) +
  theme(legend.position = "bottom", legend.key.width = unit(0.2, "cm"))
  
plot1 + plot2
```

![](p8105_hw3_rh2916_files/figure-gfm/unnamed-chunk-16-1.png)<!-- -->

For 0-100 snowfall, the most snowfall appears is 25 mm while the least
is 100 mm. The trend is decreasing from 0 to 100 although there are
fluctuations. In 0-15 mm, there are significant differences between
years.
