p8105\_hw3\_rh2916
================
Rui Huang
October 6, 2018

## Problem 1

``` r
library(p8105.datasets)
data(brfss_smart2010)
```

``` r
problem1_data =
  brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health" ) %>%
  mutate(response = as.factor(response)) %>%
  select(-class, -topic, -question, -sample_size, -(confidence_limit_low : geo_location))
```

### In 2002, which states were observed at 7 locations?

``` r
problem1_data %>%
  filter(year == 2002) %>%
  group_by(locationabbr) %>%
  summarize(n_locationdesc = n_distinct(locationdesc)) %>%
  filter(n_locationdesc == 7)
```

    ## # A tibble: 3 x 2
    ##   locationabbr n_locationdesc
    ##   <chr>                 <int>
    ## 1 CT                        7
    ## 2 FL                        7
    ## 3 NC                        7

Based on the result, CT, FL and NC were observed at 7 locations in
2002.

### Make a “spaghetti plot” that shows the number of observations in each state from 2002 to 2010.

``` r
problem1_data %>%
  group_by(year, locationabbr) %>%
  summarize(n_obs = n()) %>%
  ggplot(aes(x = year, y = n_obs, color = locationabbr)) +
  geom_line() +
  labs(
    title = "Spaghetti plot for number of observations in each state",
    x = "Year",
    y = "Number of observations",
    caption = "Data from the rnoaa package"
  ) 
```

![](p8105_hw3_rh2916_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->

Based on the result, we can find that the fluctuation for observations
increases by year. And the maximum observation number is about 220,
which occurs in 2007. The majority of observation numbers are under 100.
However, it is difficult to tell the observaion differences between
different states by the spaghetti
plot.

### Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State.

``` r
knitr::kable(
  problem1_data %>%
  spread(key = response, value = data_value) %>%
  janitor::clean_names() %>%
  filter((year == 2002 | year == 2006 |year == 2010) & locationabbr == "NY") %>%
  group_by(year) %>%
  summarize(mean_prop_excellent = mean(excellent/100, na.rm = T),
            sd_prop_excellent = sd(excellent/100, na.rm = T))
)
```

| year | mean\_prop\_excellent | sd\_prop\_excellent |
| ---: | --------------------: | ------------------: |
| 2002 |             0.2404000 |           0.0448642 |
| 2006 |             0.2253333 |           0.0400083 |
| 2010 |             0.2270000 |           0.0356721 |

From the table we can find that the mean proportion of excellent in 2002
is higher than 2006 and 2010, while the sd of proportion of excellent in
2002 is also the highest. The mean proportion of excellent in 2006 nd
2010 are
similar.

### For each year and state, compute the average proportion in each response category. Make a five-panel plot that shows the distribution of these state-level averages over time.

``` r
problem1_data %>%
  group_by(year, locationabbr, response) %>%
  summarise(state_mean = mean(data_value)/100) %>%
  ggplot(aes(x = year, y = state_mean, color = response)) +
  geom_point() +
  geom_smooth(se = F) +
  facet_grid(. ~response) +
  theme(legend.position = "bottom",
        axis.text.x = element_text(angle = 90, hjust = 1)) +
  labs(
    title = "Distribution of state-level average responses over time",
    x = "Year",
    y = "State-level average responses"
  )
```

    ## `geom_smooth()` using method = 'loess' and formula 'y ~ x'

    ## Warning: Removed 21 rows containing non-finite values (stat_smooth).

    ## Warning: Removed 21 rows containing missing values (geom_point).

![](p8105_hw3_rh2916_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

From the plot, we can find that for each year the proportion for very
good is the highest an the proportion for poor is the lowest. The
proportion distribution for this 9 years are similar.

## Problem 2

``` r
library(p8105.datasets)
data(instacart)
Problem2_data = 
  instacart %>%
  janitor::clean_names()
mode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
}
```

### To that end, write a short description of the dataset, noting the size and structure of the data.

The size of the dataset is 1384617, 15, containing 1384617 rows
(observations) and 15 columns (variables), which are order\_id,
product\_id, add\_to\_cart\_order, reordered user\_id eval\_set
order\_number order\_dow order\_hour\_of\_day,
days\_since\_prior\_order, product\_name, aisle\_id, department\_id,
aisle and department. This dataset contains integer, character variable.
There is information on users, orders, products, thus, the key variables
are product\_name(product\_id), aisle(aisle\_id) and
department(department\_id). From the key variables, we can tell the
information about each order product. For instance, we can know that the
most order is Banana for 18726, it is “fresh fruits” and is from
department “produce”.
